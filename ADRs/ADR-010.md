# ADR-010 — Observability Stack Choice (Metrics, Logs, Traces via OpenTelemetry + Prometheus + Loki)

**Status:** Accepted
**Decision Date:** 2025-11-29
**Authors:** Abhishek

## Context

The orchestrator must provide **full observability** across its distributed components:

* API Orchestrator
* Scheduler
* Worker pool
* Queue consumers
* Runner adapters (EC2, EKS, On-Prem)

Key requirements:

* **Debugging:** root-cause failures (bench issues, runner errors, retries)
* **Capacity planning:** bench utilization, queue latency, worker load
* **Performance:** execution duration, provision latency, S3 upload times
* **Cost-awareness:** cloud usage (S3 traffic, EC2 warm pool, EKS pods)
* **Compliance:** audit logs and historical traces

The system must expose **three observability pillars**:

1. **Metrics** (low-cardinality counters, gauges, histograms)
2. **Structured Logs** (machine-parseable, correlation_id aware)
3. **Distributed Tracing** (end-to-end execution tracing)

The solution must unify local development, dev/staging, and production.

## Requirements

* End-to-end tracing across API → Scheduler → Queue → Worker → Runner
* Standardized JSON logs with correlation_id and execution_id
* Metrics for queue length, failures, task durations, bench health
* Dashboards per component
* Alerting for queue backlog, bench offline, high failure rate
* Integration with Kubernetes, EC2, and on-prem adapters
* Must work with on-prem & cloud benches
* Minimal vendor lock-in
* Must support trace links to logs & metrics

## Options

### Option A — OpenTelemetry + Prometheus + Loki + Grafana (Recommended)

* OpenTelemetry SDK for traces + metrics
* Prometheus for scraping metrics
* Loki for log aggregation
* Grafana for dashboards

### Option B — AWS-native stack (X-Ray + CloudWatch + CloudWatch Metrics)

* Tight AWS integration
* Limited portability
* Harder hybrid (on-prem + cloud) support

### Option C — ELK Stack (Elasticsearch + Logstash + Kibana)

* Powerful but heavyweight
* Expensive at scale

## Decision

Adopt **Option A**:
**OpenTelemetry + Prometheus + Loki + Grafana**
as the unified observability platform across environments.

## Rationale

### Why OpenTelemetry?

* Vendor-neutral open standard
* Single SDK → traces + metrics + logs
* Easy to export to multiple backends
* Works across Python, containers, on-prem runners
* Integrates with FastAPI, boto3, aiohttp, Kubernetes, asyncio

### Why Prometheus?

* Best for metrics scraping
* Kubernetes-native
* Strong alerting rules & time-series performance
* Integrates easily with Grafana

### Why Loki?

* Scalable, cheap, log-structured storage
* Perfect fit for structured JSON logs
* Easy correlation with Grafana dashboards
* No heavy indexing like ELK

### Why not AWS X-Ray / CloudWatch-only?

* Hard to support on-prem + EKS + EC2 + hybrid runners uniformly
* No local dev-friendly equivalent
* Limited export/migration flexibility

### Why not ELK?

* Too heavy for this project
* Expensive and operationally complex

## Observability Architecture

```
API --(OTEL)--> OTEL Collector ---->
                                     ├── Prometheus (metrics)
                                     ├── Loki (logs)
                                     └── Tempo/Jaeger (traces, optional)
Scheduler --(OTEL)--> OTEL Collector
Worker ----(OTEL)--> OTEL Collector
Runner ----(OTEL)--> OTEL Collector
```

## What Gets Instrumented?

### API

* request latency
* request count
* error rate
* correlation_id in logs
* trace_id propagation

### Scheduler

* queue push/pop latency
* scheduling delay
* bench allocation latency

### Worker

* task duration (provision, run, collect, teardown)
* retry count
* failure count
* DLQ entries
* runner error metadata

### Runners

* EC2:

  * instance_ready_latency
  * ssm_execution_time
* EKS:

  * pod_startup_time
* On-prem:

  * bench_ready_time
  * bench_health_failures

### Infrastructure

* queue_length
* bench_heartbeat_age
* active_workers
* S3 upload latency

## Log Format (JSON)

Every log line must contain:

```
{
  "timestamp": "...",
  "level": "INFO|WARN|ERROR",
  "message": "...",
  "component": "api|scheduler|worker|adapter",
  "execution_id": "...",
  "task_id": "...",
  "correlation_id": "...",
  "trace_id": "...",
  "span_id": "...",
  "context": {...}
}
```

Logs → Loki → Grafana panels

## Tracing

* OpenTelemetry auto-instrumentation for FastAPI, boto3, SQL, aiohttp
* Custom spans for:

  * provision(), run_test(), collect_artifacts(), teardown()
  * queue operations
  * retry logic

Traces visualized in Grafana Tempo/Jaeger.

## Metrics

Collected via Prometheus:

* `provision_latency_seconds`
* `execution_duration_seconds`
* `task_retry_count`
* `artifact_upload_bytes_total`
* `queue_backlog`
* `bench_utilization_percent`
* `worker_busy_time_percent`

## Alerts

### Critical

* Queue backlog > threshold
* Bench offline > grace period
* Worker failure rate > threshold
* S3 upload errors recurring
* Provision latency degradation

### Warning

* Increased retry counts
* Slow EKS pod startup times
* Warm pool depletion

## Implementation Plan

### Sprint 3 (prep)

* Add OTEL SDK integration stubs in API and Worker
* Add structured logging format

### Sprint 4 (primary)

* Deploy OTEL Collector
* Add Prometheus metrics & Grafana dashboards
* Add Loki + log shipping
* Full E2E tracing

### Sprint 5

* Add security/log retention
* Integrate cost-analysis metrics

## Acceptance Criteria

* All components emit structured logs
* Traces propagate from API → scheduler → worker → runner
* Prometheus scrapes all relevant metrics
* Grafana dashboard templates created for:

  * bench utilization
  * execution timelines
  * queue health
  * runner performance
* Loki captures all logs with correlation_id & execution_id
* All telemetry deployable via Helm/IaC

## Consequences

* Excellent troubleshooting & debugging experience
* Hybrid cloud/on-prem visibility
* Easy scaling and low operational burden
* Slightly higher complexity in application instrumentation
* Run-time overhead manageable (<5%)

---

**Summary:**
Adopt **OpenTelemetry + Prometheus + Loki + Grafana** as the unified observability stack across all orchestrator components.
This ensures full metrics, logs, and tracing coverage, supports hybrid runners, and provides production-ready debugging and monitoring capabilities.
