# ADR-003 — Persistence Choice: **DynamoDB (primary)** vs **Postgres (secondary/analytics)**

**Status:** Accepted
**Decision Date:** 2025-11-29
**Authors:** Abhishek

## Context

The orchestrator needs a durable, highly available system-of-record for reservations, executions, tasks, and audit/history with the following requirements:

* Low-latency reads/writes for control-plane operations (reservations, state transitions)
* Horizontal scalability to support concurrent users and large task volumes
* Strong support for optimistic concurrency and conditional updates
* Immutable audit/history for compliance and debugging
* Retention/archival policies (cold storage to S3/Glacier)
* Easy local developer experience and deterministic tests
* Ability to support analytics/reporting (ad-hoc queries, joins) in future without impacting control-plane performance

Two leading options evaluated:

* **Option A — DynamoDB** (NoSQL managed by AWS)
* **Option B — PostgreSQL (RDS)** (relational DB)

## Decision

Use **DynamoDB as the primary system-of-record** for reservation/execution state and real-time operations. Provide a **regular export/stream** (DynamoDB Streams → Kinesis/Lambda) to **Postgres/analytics store** (or Redshift) for ad-hoc queries, reports, and historical analytics.

## Rationale

* **Scalability & availability:** DynamoDB offers seamless horizontal scaling and high availability without operational DB sharding or capacity planning overhead.
* **Control-plane performance:** Low-latency single-digit-ms writes/reads at scale are critical for the scheduler and worker concurrency goals.
* **Conditional writes & optimistic concurrency:** DynamoDB supports conditional updates and transactions, enabling idempotent state transitions and conflict detection.
* **Operational simplicity:** Managed service reduces operational burden for SLA targets; backups, TTL, and point-in-time recovery are available.
* **Auditability & immutability:** Use append-only item patterns and DynamoDB Streams to capture immutable history and feed archival pipelines to S3.
* **Separation of concerns:** Offloading analytical queries to a separate Postgres/warehouse prevents heavy analytical queries from impacting control-plane latency.

## Pros / Cons

### DynamoDB — Pros

* Managed, serverless scaling; low ops overhead.
* Fine-grained conditional writes and transactions.
* TTL and Streams simplify retention and change-data-capture.
* Well-suited for high write-throughput workflows (task queues, heartbeats).

### DynamoDB — Cons / Mitigations

* Query patterns must be designed up-front (denormalized schema).
  *Mitigation:* Design access patterns (reservations by user, executions by id/status, bench lookup) and implement GSIs for common queries.
* Complex joins/aggregations are expensive or not supported.
  *Mitigation:* Export data to Postgres/warehouse for analytics.
* Cost model requires monitoring (RCU/WCU or on-demand costs).
  *Mitigation:* Use on-demand or autoscaling; emit cost metrics; integrate with cloud-cost-optimizer in Sprint 5.

### PostgreSQL — Pros (if used for analytics/secondary)

* Rich query capabilities, transactional integrity, joins, and indexes for analytics.
* Easier ad-hoc querying and BI tools integration.

### PostgreSQL — Cons (as primary)

* Operational complexity at scale, manual sharding/replication trade-offs.
* Potential control-plane performance bottlenecks under bursty load.

## Implementation Notes

### Primary Model (DynamoDB)

* **Tables**

  * `Executions` — PK: `execution_id` (UUID); SK: `entity_type` or versioning attribute for history; attributes: status, reservation_id, commit_sha, artifacts_uri, duration, metadata.
  * `Reservations` — PK: `reservation_id`; attributes: user_id, bench_id, start_ts, end_ts, status.
  * `Benches` — PK: `bench_id`; attributes: type, capabilities, state, last_heartbeat.
  * `Tasks` — PK: `task_id`; attributes: type, status, attempts, execution_id.
* **Access patterns**

  * Get execution by id (PK).
  * Query executions by reservation_id (GSI `reservation_idx` with partition=reservation_id).
  * List active reservations by bench_id (GSI `bench_reservations_idx`).
  * Query benches by capability tag (GSI `capability_idx`).
* **Concurrency**

  * Use conditional writes (`ConditionExpression`) on `version` or `state` attribute for optimistic concurrency.
  * Use DynamoDB Transactions for multi-item atomic updates where necessary.
* **History / Audit**

  * Append-only items stored in `ExecutionHistory` table or maintain `history` items with `version` SK.
  * Configure DynamoDB Streams and Lambda to write to S3 for immutable archival and to push to analytics pipeline.
* **Retention**

  * Set TTL on archival/temporary items; persist artifacts in S3 and manage lifecycle to Glacier.
* **Local dev**

  * Support DynamoDB Local for tests and CI.
* **Security**

  * Encrypt at rest (AWS-managed keys), IAM least-privilege roles, VPC endpoints for private access.
* **Backups**

  * Enable point-in-time recovery and scheduled backups; document restore runbook.

### Secondary Model (Analytics / Reporting)

* Use DynamoDB Streams → Kinesis/Data Pipeline → ETL → Postgres/Redshift for analytics.
* Periodic batch exports to Postgres for heavier joins and BI queries.
* Alternatively, on-demand exports for ad-hoc reporting.

## Migration & Interop

* Provide repository abstraction (Repository pattern) so underlying store can be swapped for tests or future DB changes.
* Implement a migration story: export DynamoDB table snapshots to S3 and import to Postgres if a full migration is required.
* For teams needing SQL-like querying during development, provide a Postgres read-replica fed by stream exports.

## Acceptance Criteria

* ADR committed to `docs/ADRs/ADR-003.md`.
* Repository pattern implemented with DynamoDB-backed adapter and an in-memory/SQLite adapter for local tests.
* Core tables created in dev environment (or IaC templates available).
* Conditional write tests demonstrating optimistic concurrency behavior.
* DynamoDB Streams pipeline stubbed in dev that exports change events to S3 (or local mock).
* Documentation for access patterns, GSIs, and expected query performance.
* Cost monitoring and autoscaling recommendations drafted for Sprint 3.

## Consequences

* Control-plane scales with minimal DBA overhead; design focus shifts to correct access-pattern modeling and GSIs.
* Analytical workloads require separate tooling and pipelines; additional infra for ETL/warehouse is necessary.
* Cost and query-pattern monitoring becomes operational responsibilities; integrate with cost-optimizer in later sprints.

---

**Summary:** DynamoDB will be the primary system-of-record for real-time orchestrator state to meet scale, availability, and low-latency requirements; export streams will populate Postgres/analytics stores for reporting and complex queries.
