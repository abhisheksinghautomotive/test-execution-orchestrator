# ADR-009 — Artifact Storage Model (S3/EFS Layout, Retention, and Access Patterns)

**Status:** Accepted
**Decision Date:** 2025-11-29
**Authors:** Abhishek

## Context

The orchestrator must store all outputs generated by test executions, including:

* Logs (stdout/stderr, runner logs, bench logs)
* Test artifacts (reports, coverage, traces, screenshots, binary dumps)
* Metadata files (execution summary, failure details)
* Attachments for JIRA/CI integrations

Requirements:

* Scalable, durable storage
* Accessible from workers, runners, and CI
* Cost-efficient for large test output volumes
* Long-term archival for compliance
* Easy retrieval by CLI and API
* Structure predictable enough for dashboards and tooling
* Supports S3 + EFS hybrid needs (local bench logs vs cloud artifacts)

Two main storage targets:

1. **Amazon S3** → default artifact store (cloud-scale, durable, cost-efficient)
2. **Amazon EFS** → optional shared filesystem for on-prem benches or runners needing shared persistent paths

The orchestrator must define a **unified directory/prefix structure**, retention rules, and metadata format.

## Requirements

* Execution → artifacts are grouped under a stable prefix
* Artifacts must be immutable
* Metadata must be queryable and retrievable via API
* Should not require shared FS except for specific runner types
* Must allow CI/download via presigned URL
* Retention policy must be configurable
* Archival to Glacier for cost control
* Artifact uploads must be idempotent
* Large logs must be streamed (not fully buffered)
* Bench-adapter-specific artifacts must follow a consistent contract

## Options

### Option A — S3 as the primary artifact store, EFS only for on-prem (Recommended)

Well-suited for cloud-native operation and large volumes.

### Option B — EFS as primary + sync to S3

High cost, unnecessary for cloud-first workloads.

### Option C — Store everything directly in DynamoDB (not suitable)

Not designed for large binary artifacts.

## Decision

Use **S3 as the primary artifact store** with a **well-defined prefix scheme**.
Use **EFS only where required** by on-prem/HIL benches or runners that need shared POSIX filesystem access.

Artifacts are always **uploaded to S3**, even if initially generated on EFS.

## Rationale

### Why S3?

* Highly scalable
* Cheap for storage, predictable lifecycle policies
* Perfect for immutable logs & artifacts
* Native presigned URLs for secure downloads
* Easy integration with CI, JIRA, observability tools
* Works across cloud runners (EC2/EKS)

### Why EFS (optional)?

* Some on-prem benches or hardware runners only support POSIX paths
* Needed for local log streaming from persistent bench mounts
* Short-lived storage only; S3 is still source of truth

### Why not EFS as primary?

* High cost per GB
* Less durable
* Not ideal for long-term archival
* Poor for multi-region scalability

## Artifact Storage Structure (S3 Prefix Layout)

```
s3://<bucket-name>/
    executions/
        <execution_id>/
            metadata.json
            logs/
                runner.log
                bench.log
                stdout.log
                stderr.log
            artifacts/
                results.json
                coverage.xml
                screenshots/
                traces/
                custom/
```

### Rules

* `execution_id` is ULID (ADR-005)
* All artifacts are immutable once uploaded
* Workers upload log streams incrementally where possible
* Metadata stored in `metadata.json` defines:

  * execution_id
  * reservation_id
  * runner type
  * start/end timestamps
  * test suite
  * result (pass/fail)

## EFS Layout (Only When Used)

```
/mnt/bench-logs/<bench_id>/<execution_id>/
    raw/
    temp/
```

Workers copy from EFS → S3, then cleanup.

## Retention & Lifecycle Policies

### Default (configurable)

* S3: retain artifacts **30–90 days**, then move to **Glacier Deep Archive**
* EFS: retain raw bench logs for **7 days**, auto-clean
* Metadata in DynamoDB: retained for **180 days** or until archived
* Execution history: appended to S3 after completion

### Why Glacier?

* Cost-efficient for long-term retention
* Rarely accessed old logs (compliance, forensic debugging)

## Access Patterns

### CLI

```
orchestrator logs <execution_id>
```

→ fetches from S3 via presigned URL

### API

`GET /executions/{id}/artifacts`
→ returns structured list of files + signed URLs

### Workers

* Upload directly using boto3 multipart upload
* Ensure idempotency via object existence checks

### CI

* Direct URL in PR comments for logs
* Attach relevant logs to JIRA

## Error Handling & Idempotency

* Workers retry S3 uploads on transient failures
* Uploads must use content hashing to avoid duplicates
* If EFS → S3 copy fails, retry with exponential backoff
* On final failure, mark execution as `artifact_error`

## Implementation Plan

### Sprint 2

* Stub out mock artifact paths only
* No S3/EFS integration yet

### Sprint 3

* S3 upload implementation in EC2/EKS runners
* EFS-integration for on-prem runner
* Metadata.json generation
* End-to-end artifact tests

### Sprint 4

* Observability added:

  * artifact_upload_duration
  * artifact_size_bytes
  * artifact_failures_total

### Sprint 5

* Glacier lifecycle policies
* Cost analytics integration (per execution storage cost)

### Sprint 6

* Scale testing for large artifact uploads
* Fault injection for partial uploads

## Acceptance Criteria

* S3 prefix structure implemented exactly as defined
* Presigned URL endpoint functional
* Workers support resumable uploads
* Artifact metadata validated and retrievable
* EFS support implemented only for on-prem runners
* Lifecycle policies documented and implemented via IaC
* End-to-end flow passing for:

  * Cloud EC2 runner
  * EKS runner
  * On-prem bench runner

## Consequences

* S3 becomes single source-of-truth for artifacts
* EFS usage minimized to reduce cost and operational complexity
* Clear, predictable artifact layout simplifies CI, CLI, dashboards, and debugging
* Lifecycle management becomes a central cost optimization tool

---

**Summary:**
Use **S3 as the primary, immutable artifact storage** with a strict prefix layout, presigned access, and lifecycle policies.
Use **EFS only where hardware benches require POSIX storage**, with periodic copying to S3.
This model scales, is cost-efficient, and integrates cleanly into the orchestrator’s execution lifecycle.
