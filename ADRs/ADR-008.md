# ADR-008 — Runner Provisioning Model (EC2 Warm Pool vs EKS Ephemeral vs On-Prem Bench)

**Status:** Accepted
**Decision Date:** 2025-11-29
**Authors:** Abhishek

## Context

The orchestrator must provision execution environments for HIL/SIL workloads.
There are three major provisioning targets:

1. **Cloud VMs (EC2)**
2. **Ephemeral containers/pods (EKS)**
3. **On-prem HIL benches** (via infra-bench-platform)

Each backend has different constraints:

* **EC2**

  * Slow cold-start times (60–120s).
  * High per-instance cost but stable compute.
  * Supports warm pools, AMIs, SSM, EBS-attached logs.

* **EKS**

  * Fast pod-start times (5–12s).
  * Good for stateless SIL workloads.
  * Native auto-scaling, ephemeral execution model.

* **On-prem HIL benches**

  * External provisioning done by the infra team.
  * Requires reservation and hardware readiness checks.
  * Cannot be created/destroyed—only allocated and released.

**Goal:**
Define a unified provisioning model that supports **all three** without modifying scheduler/worker logic.
This must work with the Runner Adapter interface from ADR-007.

## Requirements

* Allow workers to “provision” any environment in a unified way
* Provision must return a **RunnerHandle**
* Must support warm pools (reused EC2 instances)
* Must support ephemeral EKS pods launched per execution
* Must treat on-prem benches as “already provisioned” resources
* Must enable idempotent provisioning (retry-safe)
* Must support teardown semantics (EC2 terminate, Pod delete, bench release)
* Should allow for auto-scaling in the future
* Must integrate with IAM roles and network isolation policies
* Must allow for tagged bench selection (capabilities, hardware type)

## Options

### Option A — Unified Provisioning Model via Adapter Methods (Recommended)

Let each backend implement:

```
provision() → RunnerHandle
teardown(handle)
```

But each backend internally follows its own provisioning strategy:

* EC2Runner → warm pool reuse or cold-start
* EKSRunner → ephemeral pod create/delete
* OnPremBenchRunner → call infra API → mark reserved

### Option B — Central Provisioning Service

A separate provisioning microservice owns provisioning for all environments.

### Option C — Worker-Embedded Provision Logic

Workers directly call AWS APIs, Kubernetes APIs, or HIL APIs.

## Decision

Use **Option A: Unified provisioning model implemented inside each Runner Adapter**.

The orchestrator **must not** know how provisioning happens.
Each runner type encapsulates its own provisioning strategy internally.

Workers simply call:

```
handle = adapter.provision(bench_spec)
```

## Rationale

### Why Option A?

* Clean adherence to the Strategy Pattern from ADR-007
* Keeps cloud/on-prem specifics out of orchestrator logic
* Enables incremental backend implementation across sprints
* Easy to test locally (mock provisioning)
* Natural separation of concerns:

  * Scheduler = picks bench
  * Worker = executes lifecycle
  * RunnerAdapter = knows how to provision hardware/container

### Why not Option B?

* Overkill for early sprints
* Adds latency and complexity
* Harder to maintain and test

### Why not Option C?

* Tight coupling between worker and infrastructure
* Harder to mock
* Violates separation of concerns
* Risks leaking cloud details into core logic

## Provisioning Strategies

### **1. EC2Runner Provisioning (Cloud VM)**

#### Flow:

1. Check warm pool for idle instance (tagged with bench type).
2. If none available:

   * Launch new EC2 instance using pre-baked AMI.
3. Wait for `running + SSM available`.
4. Return `RunnerHandle(ec2_instance_id, ip, metadata)`.

#### Notes:

* Warm pool drastically reduces cold-start cost.
* Warm pool scaling configured via ASG policies.
* All provisioning logged with `correlation_id`.

### **2. EKSRunner Provisioning (Ephemeral Pod)**

#### Flow:

1. Create pod using runner template.
2. Attach ephemeral workspace volume.
3. Wait for pod to reach `Running`.
4. Return `RunnerHandle(pod_name, namespace, metadata)`.

#### Notes:

* Fastest provisioning method.
* Ideal for SIL workloads and lightweight test suites.

### **3. OnPremBenchRunner Provisioning**

#### Flow:

1. Call infra-bench-platform API:

```
reserve(bench_id)
```

2. Wait for bench to report “ready”.
3. Return handle tied to physical bench ID.

#### Notes:

* No true “provision” step—bench already exists.
* Failure means physical bench is unavailable.

## Teardown Model

### EC2Runner

* If warm pool model → return to pool
* If cold-start → terminate instance
* Ensure logs flushed before teardown
* SSM session cleanup

### EKSRunner

* Delete pod
* Cleanup temporary volumes

### OnPremBenchRunner

* Call:

```
release(bench_id)
```

* Update local orchestrator DB state

## Error + Retry Model

* Provision failures MUST be retried automatically (worker-level)
* All provisioning steps must be idempotent
* EC2: handle stuck instances, retry with new instance
* EKS: detect CrashLoopBackoff, recreate pod
* On-prem: retry reservation until timeout

## Implementation Plan

### Sprint 2

* Stub provisioning in LocalMockRunner (sleep + fake handle)

### Sprint 3

* Implement EC2Runner provisioning with warm pool
* Implement EKSRunner provisioning
* Implement OnPremBenchRunner reservation binding
* Integration tests using dev infra
* Update RunnerFactory to load cloud/on-prem runners
* Add provisioning metrics (duration, failures, retries)

### Future Enhancements

* Auto-scaling warm pool
* Adaptive provisioning based on usage history
* Pod priority classes for critical tests
* Support for hybrid “remote lab” provisioning

## Acceptance Criteria

* All runner adapters implement a consistent provisioning interface
* Worker never directly interacts with AWS/K8s/bench APIs
* Provisioning is idempotent across retries
* EC2 warm pool documented and working in dev/staging
* EKS pod lifecycle tested end-to-end
* On-prem bench provisioning integrated with infra-bench-platform
* Documentation added in `docs/architecture/provisioning.md`

## Consequences

* Clean modularity in lifecycle flows
* Easy addition of new provisioning backends
* Operational visibility increases (per-run metrics)
* Slightly more logic inside runner adapters, but cleaner orchestrator core

---

**Summary:**
Provisioning is handled **inside each Runner Adapter**, not centrally.
EC2 uses warm pools, EKS uses ephemeral pods, and on-prem benches use reservation APIs.
The orchestrator stays backend-agnostic, making scaling and maintenance significantly easier.
