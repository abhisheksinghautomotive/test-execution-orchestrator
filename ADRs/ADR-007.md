# ADR-007 — Runner Adapter Model (Strategy Pattern for Bench Execution Backends)

**Status:** Accepted
**Decision Date:** 2025-11-29
**Authors:** Abhishek

## Context

The orchestrator must support running tests on **multiple types of execution environments**:

* Local mock/SIL environments (Sprint 2)
* On-prem HIL benches (Sprint 3)
* Cloud EC2 runners (Sprint 3)
* EKS ephemeral pods (Sprint 3)
* Future hybrid or third-party execution systems (Sprint 4+)

Each execution backend has different capabilities, provisioning steps, teardown semantics, artifact handling, and error behaviors.

We need a **unified interface** so that the scheduler and workers can execute tests without knowing *which* backend is used.

### Requirements

* Pluggable backend implementation
* Unified interface for provision → run → collect → teardown
* Clean error handling and retry semantics
* Support for synchronous and async operations
* Isolation of infrastructure details from core orchestration logic
* Testability with fully mocked runners
* Ability to add new runner types without modifying workers

This naturally aligns with the **Strategy Pattern**.

## Options

### **Option A — Strategy Pattern (Recommended)**

Define an abstract `RunnerAdapter` with concrete implementations per runner type.

### **Option B — If/Else Logic Inside Worker**

Workers determine backend type via conditional logic.

### **Option C — Plugin Loader Framework (Dynamic Importing)**

Each runner is a plugin package loaded at runtime.

## Decision

Use **Option A (Strategy Pattern)** with a well-defined `RunnerAdapter` interface, injected at runtime based on the bench type.

Workers will call **exactly the same method signatures**, regardless of backend.

## Rationale

* **Extensibility:** New backends can be added without modifying core services.
* **Testability:** Worker tests can use simple mock runners.
* **Maintainability:** Each backend has its own isolated class + config.
* **Compatibility:** Works well with dependency injection, future plugin loading, and CI-based runners.
* **Clear contracts:** Makes error/timeout/retry behavior explicit.

Option B would introduce complex conditional branching in workers.
Option C is overkill for Sprint 2 and adds unnecessary complexity now.

## Runner Adapter Interface Design

### Core Interface (Python)

```
class RunnerAdapter(Protocol):
    def provision(self, bench_spec: dict) -> RunnerHandle: ...
    def run_test(self, handle: RunnerHandle, payload: dict) -> TestResult: ...
    def collect_artifacts(self, handle: RunnerHandle) -> ArtifactBundle: ...
    def teardown(self, handle: RunnerHandle) -> None: ...
```

### Behavior Requirements

* **Provision**
  Allocate/prepare the execution environment (EC2 instance, pod, bench reservation, etc.).
* **Run Test**
  Execute the payload; return result metadata.
* **Collect Artifacts**
  Return logs, reports, and file paths for S3 upload.
* **Teardown**
  Mandatory cleanup to avoid leaks, cost overruns, or stale state.

### RunnerHandle

Opaque identifier that tracks:

* Container/Pod ID
* EC2 instance ID
* Bench reservation ID
* Local execution path
* Environment metadata

Workers must **never inspect the internals** of the handle.

## Runner Types (Sprint Roadmap)

### Sprint 2

* **LocalMockRunner** (fully fake, deterministic)

  * Used for E2E tests
  * Generates fake logs/results
  * No external dependencies

### Sprint 3

* **EC2Runner**

  * Provisions EC2 instance or reuses warm pool
  * SSH/SSM to execute test package
  * Uploads artifacts to S3

* **EKSRunner**

  * Launches pod/job with test container
  * Streams logs from stdout
  * Collects artifacts via S3/volume mount

* **OnPremBenchRunner**

  * Integrates with infra-bench-platform API
  * Coordinates HIL hardware allocation
  * Uses persistent storage/EFS for logs

### Future

* **RemoteRunner** (for third-party labs)
* **ContainerRunner** (local container sandbox)
* **Custom hardware runners**

## Configuration Model

```
runner_config:
  type: "local_mock" | "ec2" | "eks" | "onprem"
  retries: 3
  timeout_seconds: 600
  artifacts_dir: "/var/artifacts"
```

Workers load runner adapters using a simple **factory**:

```
adapter = RunnerFactory.create(runner_type, config)
```

## Error Handling Model

### All methods must:

* Raise well-defined exceptions:

  * `ProvisionError`
  * `ExecutionError`
  * `ArtifactError`
  * `TeardownError`
* Provide retry-safe error messages
* Include `correlation_id` and `execution_id` in logs

### Worker Responsibilities:

* Convert exceptions to task retries or DLQ entries
* Never retry teardown multiple times unless safe

## Implementation Plan

### Sprint 2

* Implement abstract `RunnerAdapter`
* Implement `LocalMockRunner`
* Implement `RunnerFactory`
* Add unit tests for adapter contract
* Add integration test for mock execution lifecycle

### Sprint 3+

* Implement EC2Runner, EKSRunner, OnPremBenchRunner
* Add S3/EFS artifact handling
* Add warm pool support for EC2Runner

### Shared Tasks

* Add adapter metrics (execution duration, failure rate)
* Add OpenTelemetry spans for provision/run/collect/teardown

## Acceptance Criteria

* `RunnerAdapter` interface defined and documented
* `LocalMockRunner` fully implements adapter contract
* Worker uses adapter interface only (no backend-specific logic)
* RunnerFactory chooses correct backend using bench type
* Unit tests cover:

  * Provision → run → collect → teardown (mock)
  * Error cases & retries
* Documentation added in `docs/adapters/`

## Consequences

* Clean separation between orchestrator and execution environments
* Easier to add new backends in future
* Adapter boundary becomes a core architectural contract
* Slight abstraction overhead but highly maintainable long-term

---

**Summary:**
Define a stable, backend-agnostic `RunnerAdapter` interface and use the Strategy Pattern to support multiple execution environments (mock, EC2, EKS, on-prem). This ensures modularity, testability, and long-term extensibility of the orchestrator.
